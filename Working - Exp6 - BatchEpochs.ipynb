{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c55efec-cd56-4d73-bb39-509502d3cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# libraries for partitioning and batching the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448218c3-46f9-42c6-8016-5cc44f9fd1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a909f8-7c74-49da-9353-cb75aea01d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"default of credit card clients.csv\")\n",
    "data = df.drop('Y', axis=1)\n",
    "labels = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d210905-ed29-4181-b22e-a76be2db4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2zscore = data.keys()\n",
    "data[cols2zscore] = data[cols2zscore].apply(stats.zscore)\n",
    "\n",
    "data = torch.tensor(data.values).float()\n",
    "labels = torch.tensor(labels.values).float()\n",
    "\n",
    "#X = data\n",
    "#y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499f56fe-63b5-4cff-9e6b-f6a7aa289f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize = 0.75\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=trainsize)\n",
    "train_dataDataset = TensorDataset(train_data, train_labels)\n",
    "test_dataDataset = TensorDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80300ec6-246d-4dce-ad2d-c6d17c1442dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf4fa4-6c25-4236-a4f5-09ca1508c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "230995cb-ccac-435e-a1eb-925f50c94315",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 8192\n",
    "# train and test dataloaders\n",
    "test_loader  = DataLoader(test_dataDataset,batch_size=test_dataDataset.tensors[0].shape[0])\n",
    "train_loader = DataLoader(train_dataDataset,batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca5c9115-052f-485f-9829-a1c201aa9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b73a95-adc8-4d2b-98a4-23e2c291dfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cd75f-f668-487e-b6ed-e8fa72a76096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d224ad4-901b-4a45-96bf-3779eb3c7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createANNmodel():\n",
    "\n",
    "  # model architecture\n",
    "  ANN = nn.Sequential(\n",
    "      nn.Linear(23,64),  # input layer\n",
    "      nn.ReLU(),        # activation unit\n",
    "      nn.Linear(64,16),  # hidden layer\n",
    "      nn.ReLU(),        # activation unit\n",
    "      nn.Linear(16,1),   # output unit\n",
    "      nn.Sigmoid(),     # final activation unit\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.BCELoss() # but better to use BCEWithLogitsLoss\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANN.parameters(),lr=0.1)\n",
    "\n",
    "  # model output\n",
    "  return ANN,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed72345c-e5c8-4a57-8684-b74e54172338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global parameter 'numepochs' with a value of 500, representing the number of training epochs.\n",
    "numepochs = 1000\n",
    "\n",
    "# Define a function 'trainTheModel' to train the neural network.\n",
    "def trainTheModel():\n",
    "\n",
    "  # Initialize lists to store training and test accuracies, as well as losses during training.\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "  losses   = []\n",
    "\n",
    "  # Loop over epochs (training iterations).\n",
    "  for epochi in range(numepochs):\n",
    "    \n",
    "    if epochi % 100 == 0:\n",
    "        print(epochi)\n",
    "    # Activate training mode for the neural network.\n",
    "    ANN.train()\n",
    "\n",
    "    # Initialize lists to store accuracy and loss for each batch during training.\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "\n",
    "    # Loop over training data batches.\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # Perform a forward pass through the neural network.\n",
    "      yHat = ANN(X)\n",
    "      #print(yHat)\n",
    "      # Calculate the loss using the specified loss function.\n",
    "      loss = lossfun(yHat,y)\n",
    "      \n",
    "      # Clear previous gradient calculations.\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Perform backpropagation to compute gradients.\n",
    "      loss.backward()\n",
    "      \n",
    "      # Update the model's parameters using the optimizer.\n",
    "      optimizer.step()\n",
    "\n",
    "      # Compute training accuracy for this batch and append to 'batchAcc'.\n",
    "      #batchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "      batchAcc.append( 100*torch.mean(((yHat.round()) == y).float()) )\n",
    "      \n",
    "      # Append the loss value for this batch to 'batchLoss'.\n",
    "      batchLoss.append( loss.item() )\n",
    "    # End of batch loop...\n",
    "\n",
    "    # Calculate the average training accuracy for this epoch and append to 'trainAcc'.\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # Append the average loss for this epoch to 'losses'.\n",
    "    losses.append( np.mean(batchLoss) )\n",
    "\n",
    "    # Test accuracy on the validation set.\n",
    "    \n",
    "    # Extract X,y from the test dataloader.\n",
    "    X,y = next(iter(test_loader))\n",
    "\n",
    "    # Activate evaluation mode (no gradient computation) for the neural network.\n",
    "    ANN.eval()\n",
    "\n",
    "    # Perform forward pass for the test data.\n",
    "    with torch.no_grad():\n",
    "      #predictions = torch.argmax( ANN(X),axis=1 )\n",
    "      predictions = ANN(X)\n",
    "      #print(predictions)\n",
    "    \n",
    "    # Compute accuracy on the test data and append to 'testAcc'.\n",
    "    #testAcc.append( 100*torch.mean((predictions == y).float()).item() )\n",
    "    testAcc.append( 100*torch.mean(((predictions.round()) == y).float()) )\n",
    "    #rint(predictions)\n",
    "    \n",
    "  \n",
    "  # Function output: training accuracies, test accuracies, and losses over epochs.\n",
    "  return trainAcc, testAcc, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea3367f-139d-437a-821a-8e8274f075aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m ANN, lossfun, optimizer = createANNmodel()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Train the model using the current batch size.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m trainAcc, testAcc, losses = \u001b[43mtrainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrainTheModel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     54\u001b[39m losses.append( np.mean(batchLoss) )\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Test accuracy on the validation set.\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Extract X,y from the test dataloader.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m X,y = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Activate evaluation mode (no gradient computation) for the neural network.\u001b[39;00m\n\u001b[32m     62\u001b[39m ANN.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create a new model, loss function, and optimizer.\n",
    "ANN, lossfun, optimizer = createANNmodel()\n",
    "# Train the model using the current batch size.\n",
    "trainAcc, testAcc, losses = trainTheModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef58b2-6d3d-4437-b110-fa8bf8daa1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(ANN, (1,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6976d6-580f-4a51-b0b4-6e33750ee06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with train size=' + str(trainsize) + ' & batch size of=' + str(batchsize))\n",
    "\n",
    "ax[1].plot(trainAcc,'ro-')\n",
    "ax[1].plot(testAcc,'bs-')\n",
    "ax[1].set_title('Accuracy with train size=' + str(trainsize) + ' & batch size of=' + str(batchsize))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([70,100])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb22c41-01bb-4113-bc45-fefc8b50921a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
